---
title: Dimension-free convergence rates for gradient Langevin dynamics in RKHS
abstract: Gradient Langevin dynamics (GLD) and stochastic GLD (SGLD) have attracted
  considerable attention lately, as a way to provide convergence guarantees in a non-convex
  setting. However, the known rates grow exponentially with the dimension of the space
  under the dissipative condition. In this work, we provide a convergence analysis
  of GLD and SGLD when the optimization space is an infinite-dimensional Hilbert space.
  More precisely, we derive non-asymptotic, dimension-free convergence rates for GLD/SGLD
  when performing regularized non-convex optimization in a reproducing kernel Hilbert
  space. Amongst others, the convergence analysis relies on the properties of a stochastic
  differential equation, its discrete time Galerkin approximation and the geometric
  ergodicity of the associated Markov chains.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: muzellec22a
month: 0
tex_title: Dimension-free convergence rates for gradient Langevin dynamics in RKHS
firstpage: 1356
lastpage: 1420
page: 1356-1420
order: 1356
cycles: false
bibtex_author: Muzellec, Boris and Sato, Kanji and Massias, Mathurin and Suzuki, Taiji
author:
- given: Boris
  family: Muzellec
- given: Kanji
  family: Sato
- given: Mathurin
  family: Massias
- given: Taiji
  family: Suzuki
date: 2022-06-28
address:
container-title: Proceedings of Thirty Fifth Conference on Learning Theory
volume: '178'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v178/muzellec22a/muzellec22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
