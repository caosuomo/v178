---
title: 'Horizon-Free Reinforcement Learning in Polynomial Time: the Power of Stationary
  Policies'
abstract: This paper gives the first polynomial-time algorithm for tabular Markov
  Decision Processes (MDP) that enjoys a regret bound \emph{independent on the planning
  horizon}.  Specifically, we consider tabular MDP with $S$ states, $A$ actions, a
  planning horizon $H$, total reward bounded by $1$, and the agent plays for $K$ episodes.
  We design an algorithm that achieves an  $O\left(\mathrm{poly}(S,A,\log K)\sqrt{K}\right)$
  regret in contrast to existing bounds which either has an additional $\mathrm{polylog}(H)$
  dependency \citep{zhang2020reinforcement} or has an exponential dependency on $S$ \citep{li2021settling}.
  Our result relies on a sequence of new structural lemmas establishing the approximation
  power, stability, and concentration property of stationary policies, which can have
  applications in other problems related to Markov chains.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhang22a
month: 0
tex_title: 'Horizon-Free Reinforcement Learning in Polynomial Time: the Power of Stationary
  Policies'
firstpage: 3858
lastpage: 3904
page: 3858-3904
order: 3858
cycles: false
bibtex_author: Zhang, Zihan and Ji, Xiangyang and Du, Simon
author:
- given: Zihan
  family: Zhang
- given: Xiangyang
  family: Ji
- given: Simon
  family: Du
date: 2022-06-28
address:
container-title: Proceedings of Thirty Fifth Conference on Learning Theory
volume: '178'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v178/zhang22a/zhang22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
