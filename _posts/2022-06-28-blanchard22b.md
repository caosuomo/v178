---
title: 'Universal Online Learning: an Optimistically Universal Learning Rule'
abstract: We study the subject of universal online learning with non-i.i.d. processes
  for bounded losses. The notion of universally consistent learning was defined by
  Hanneke in an effort to study learning theory under minimal assumptions, where the
  objective is to obtain low long-run average loss for any target function. We are
  interested in characterizing processes for which learning is possible and whether
  there exist learning rules guaranteed to be universally consistent given the only
  assumption that such learning is possible. The case of unbounded losses is very
  restrictive since the learnable processes almost surely have to visit a finite number
  of points and as a result, simple memorization is optimistically universal. We focus
  on the bounded setting and give a complete characterization of the processes admitting
  strong and weak universal learning. We further show that the k-nearest neighbor
  algorithm (kNN) is not optimistically universal and present a novel variant of 1NN
  which is optimistically universal for general input and value spaces in both strong
  and weak settings. This closes all the COLT 2021 open problems posed on universal
  online learning.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: blanchard22b
month: 0
tex_title: 'Universal Online Learning: an Optimistically Universal Learning Rule'
firstpage: 1077
lastpage: 1125
page: 1077-1125
order: 1077
cycles: false
bibtex_author: Blanchard, Moise
author:
- given: Moise
  family: Blanchard
date: 2022-06-28
address:
container-title: Proceedings of Thirty Fifth Conference on Learning Theory
volume: '178'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v178/blanchard22b/blanchard22b.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
