---
title: Statistical Estimation and Online Inference via Local SGD
abstract: 'We analyze the novel Local SGD  in federated Learning, a multi-round estimation
  procedure that uses intermittent communication to improve communication efficiency.
  Under a $2{+}\delta$ moment condition on stochastic gradients, we first establish
  a {\it functional central limit theorem} that shows the averaged iterates of Local
  SGD converge weakly to a rescaled Brownian motion. We next provide two iterative
  inference methods: the {\it plug-in} and the {\it random scaling}. Random scaling
  constructs an asymptotically pivotal statistic for inference by using the information
  along the whole Local SGD path. Both the methods are communication efficient and
  applicable to online data. Our results show that Local SGD simultaneously achieves
  both statistical efficiency and communication efficiency.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: li22b
month: 0
tex_title: Statistical Estimation and Online Inference via Local SGD
firstpage: 1613
lastpage: 1661
page: 1613-1661
order: 1613
cycles: false
bibtex_author: Li, Xiang and Liang, Jiadong and Chang, Xiangyu and Zhang, Zhihua
author:
- given: Xiang
  family: Li
- given: Jiadong
  family: Liang
- given: Xiangyu
  family: Chang
- given: Zhihua
  family: Zhang
date: 2022-06-28
address:
container-title: Proceedings of Thirty Fifth Conference on Learning Theory
volume: '178'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v178/li22b/li22b.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
