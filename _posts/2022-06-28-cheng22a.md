---
title: 'Memorize to generalize: on the necessity of interpolation in high dimensional
  linear regression'
abstract: We examine the necessity of interpolation in overparameterized models, that
  is, when achieving optimal predictive risk in machine learning problems requires
  (nearly) interpolating the training data. In particular, we consider simple overparameterized
  linear regression $y = X \theta + w$ with random design $X \in \real^{n \times d}$
  under the proportional asymptotics $d/n \to \gamma \in (1, \infty)$.  We precisely
  characterize how prediction (test) error necessarily scales with training error
  in this setting.  An implication of this characterization is that as the label noise
  variance $\sigma^2 \to 0$, any estimator that incurs at least $\mathsf{c}\sigma^4$
  training error for some constant $\mathsf{c}$ is necessarily suboptimal and  will
  suffer growth in excess prediction error at least linear in the training error.
  Thus, optimal performance requires fitting training data to substantially higher
  accuracy than the inherent noise floor of the problem.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: cheng22a
month: 0
tex_title: 'Memorize to generalize: on the necessity of interpolation in high dimensional
  linear regression'
firstpage: 5528
lastpage: 5560
page: 5528-5560
order: 5528
cycles: false
bibtex_author: Cheng, Chen and Duchi, John and Kuditipudi, Rohith
author:
- given: Chen
  family: Cheng
- given: John
  family: Duchi
- given: Rohith
  family: Kuditipudi
date: 2022-06-28
address:
container-title: Proceedings of Thirty Fifth Conference on Learning Theory
volume: '178'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v178/cheng22a/cheng22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
