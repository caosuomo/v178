---
title: 'The merged-staircase property: a necessary and nearly sufficient condition
  for SGD learning of sparse functions on two-layer neural networks'
abstract: 'It is currently known how to characterize functions that neural networks
  can learn with SGD for two extremal parametrizations: neural networks in the linear
  regime, and neural networks with no structural constraints. However, for the main
  parametrization of interest —non-linear but regular networks— no tight characterization
  has yet been achieved, despite significant developments. We take a step in this
  direction by considering depth-2 neural networks trained by SGD in the mean-field
  regime. We consider functions on binary inputs that depend on a latent low-dimensional
  subspace (i.e., small number of coordinates). This regime is of interest since it
  is poorly understood how neural networks routinely tackle high-dimensional datasets
  and adapt to latent low-dimensional structure without suffering from the curse of
  dimensionality. Accordingly, we study SGD-learnability with $O(d)$ sample complexity
  in a large ambient dimension $d$.  Our main results characterize a hierarchical
  property —the merged-staircase property— that is both \emph{necessary and nearly
  sufficient} for learning in this setting.  We further show that non-linear training
  is necessary: for this class of functions, linear methods on any feature map (e.g.,
  the NTK) are not capable of learning efficiently. The key tools are a new “dimension-free”
  dynamics approximation result that applies to functions defined on a latent space
  of low-dimension, a proof of global convergence based on polynomial identity testing,
  and an improvement of lower bounds against linear methods for non-almost orthogonal
  functions.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: abbe22a
month: 0
tex_title: 'The merged-staircase property: a necessary and nearly sufficient condition
  for SGD learning of sparse functions on two-layer neural networks'
firstpage: 4782
lastpage: 4887
page: 4782-4887
order: 4782
cycles: false
bibtex_author: Abbe, Emmanuel and Adsera, Enric Boix and Misiakiewicz, Theodor
author:
- given: Emmanuel
  family: Abbe
- given: Enric Boix
  family: Adsera
- given: Theodor
  family: Misiakiewicz
date: 2022-06-28
address:
container-title: Proceedings of Thirty Fifth Conference on Learning Theory
volume: '178'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v178/abbe22a/abbe22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
