---
title: Accelerated SGD for Non-Strongly-Convex Least Squares
abstract: We consider stochastic approximation for the least squares regression problem
  in the non-strongly convex setting. We present the first practical algorithm that
  achieves the optimal prediction error rates in terms of dependence on the noise
  of the problem, as $O(d/t)$ while accelerating the forgetting of the initial conditions
  to $O(d/t^2)$. Our new algorithm is based on a simple modification of the accelerated
  gradient descent. We provide convergence results for both the averaged and the last
  iterate of the algorithm. In order to describe the tightness of these new bounds,
  we present a matching lower bound in the noiseless setting and thus show the optimality
  of our algorithm.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: varre22a
month: 0
tex_title: Accelerated SGD for Non-Strongly-Convex Least Squares
firstpage: 2062
lastpage: 2126
page: 2062-2126
order: 2062
cycles: false
bibtex_author: Varre, Aditya and Flammarion, Nicolas
author:
- given: Aditya
  family: Varre
- given: Nicolas
  family: Flammarion
date: 2022-06-28
address:
container-title: Proceedings of Thirty Fifth Conference on Learning Theory
volume: '178'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v178/varre22a/varre22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
