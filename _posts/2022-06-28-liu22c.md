---
title: Learning GMMs with Nearly Optimal Robustness Guarantees
abstract: In this work we solve the problem of robustly learning a high-dimensional
  Gaussian mixture model with $k$ components from $\epsilon$-corrupted samples up
  to accuracy $\widetilde{O}(\epsilon)$ in total variation distance for any constant
  $k$ and with mild assumptions on the mixture. This robustness guarantee is optimal
  up to polylogarithmic factors. The main challenge is that most earlier works rely
  on learning individual components in the mixture, but this is impossible in our
  setting, at least for the types of strong robustness guarantees we are aiming for.
  Instead we introduce a new framework which we call {\em strong observability} that
  gives us a route to circumvent this obstacle.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: liu22c
month: 0
tex_title: Learning GMMs with Nearly Optimal Robustness Guarantees
firstpage: 2815
lastpage: 2895
page: 2815-2895
order: 2815
cycles: false
bibtex_author: Liu, Allen and Moitra, Ankur
author:
- given: Allen
  family: Liu
- given: Ankur
  family: Moitra
date: 2022-06-28
address:
container-title: Proceedings of Thirty Fifth Conference on Learning Theory
volume: '178'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v178/liu22c/liu22c.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
