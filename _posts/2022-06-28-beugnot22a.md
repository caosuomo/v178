---
title: On the Benefits of Large Learning Rates for Kernel Methods
abstract: This paper studies an intriguing phenomenon related to the good generalization
  performance of estimators obtained by using large learning rates within gradient
  descent algorithms. First observed in the deep learning literature, we show that
  such a phenomenon can be precisely characterized in the context of kernel methods,
  even though the resulting optimization problem is convex. Specifically, we consider
  the minimization of a quadratic objective in a separable Hilbert space, and show
  that with early stopping, the choice of learning rate influences the spectral decomposition
  of the obtained solution on the Hessianâ€™s eigenvectors. This extends an intuition
  described by Nakkiran (2020) on a two-dimensional toy problem to realistic learning
  scenarios such as kernel ridge regression. While large learning rates may be proven
  beneficial as soon as there is a mismatch between the train and test objectives,
  we further explain why it already occurs in classification tasks without assuming
  any particular mismatch between train and test data distributions.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: beugnot22a
month: 0
tex_title: On the Benefits of Large Learning Rates for Kernel Methods
firstpage: 254
lastpage: 282
page: 254-282
order: 254
cycles: false
bibtex_author: Beugnot, Gaspard and Mairal, Julien and Rudi, Alessandro
author:
- given: Gaspard
  family: Beugnot
- given: Julien
  family: Mairal
- given: Alessandro
  family: Rudi
date: 2022-06-28
address:
container-title: Proceedings of Thirty Fifth Conference on Learning Theory
volume: '178'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v178/beugnot22a/beugnot22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
