---
title: Minimax Regret on Patterns Using Kullback-Leibler Divergence Covering
abstract: This paper considers the problem of finding a tighter upper bound on the
  minimax regret of patterns, a class used to study large-alphabet distributions which
  avoids infinite asymptotic regret and redundancy. Our method for finding upper bounds
  for minimax regret uses cover numbers with Kullback-Leibler (KL) divergence as the
  distance. Compared to existing results by Acharya et al. (2013), we are able to
  improve the power of the exponent on the logarithmic term, giving a minimax regret
  bound which matches the best known minimax redundancy bound on patterns.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: tang22a
month: 0
tex_title: Minimax Regret on Patterns Using Kullback-Leibler Divergence Covering
firstpage: 3095
lastpage: 3112
page: 3095-3112
order: 3095
cycles: false
bibtex_author: Tang, Jennifer
author:
- given: Jennifer
  family: Tang
date: 2022-06-28
address:
container-title: Proceedings of Thirty Fifth Conference on Learning Theory
volume: '178'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v178/tang22a/tang22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
