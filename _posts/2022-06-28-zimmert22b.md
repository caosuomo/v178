---
title: 'Return of the bias: Almost minimax optimal high probability bounds for adversarial
  linear bandits'
abstract: We introduce a modification of follow the regularised leader and combine
  it with the log determinant potential and suitable loss estimators to prove that
  the minimax regret for adaptive adversarial linear bandits is at most $O(d \sqrt{T
  \log(T)})$ where $d$ is the dimension and $T$ is the number of rounds.  By using
  exponential weights, we improve this bound to $O(\sqrt{dT\log(kT)})$ when the action
  set has size $k$. These results confirms an old conjecture. We also show that follow
  the regularized leader with the entropic barrier and suitable loss estimators has
  regret against an adaptive adversary of at most $O(d^2 \sqrt{T} \log(T))$ and can
  be implement in polynomial time, which improves on the best known bound for an efficient
  algorithm of $O(d^{7/2} \sqrt{T} \poly(\log(T)))$ by Lee et al 2020.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zimmert22b
month: 0
tex_title: 'Return of the bias: Almost minimax optimal high probability bounds for
  adversarial linear bandits'
firstpage: 3285
lastpage: 3312
page: 3285-3312
order: 3285
cycles: false
bibtex_author: Zimmert, Julian and Lattimore, Tor
author:
- given: Julian
  family: Zimmert
- given: Tor
  family: Lattimore
date: 2022-06-28
address:
container-title: Proceedings of Thirty Fifth Conference on Learning Theory
volume: '178'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v178/zimmert22b/zimmert22b.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
