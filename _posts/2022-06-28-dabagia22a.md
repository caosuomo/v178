---
title: Assemblies of neurons learn to classify well-separated distributions
abstract: An assembly is a large population of neurons whose synchronous firing represents
  a memory, concept, word, and other cognitive category.  Assemblies are believed
  to provide a bridge between high-level cognitive phenomena and low-level neural
  activity. Recently, a computational system called the \emph{Assembly Calculus} (AC),
  with a repertoire of biologically plausible operations on assemblies, has been shown
  capable of simulating arbitrary space-bounded computation, but also of simulating
  complex cognitive phenomena such as language, reasoning, and planning.  However,
  the mechanism whereby assemblies can mediate {\em learning} has not been known.  Here
  we present such a mechanism, and prove rigorously that, for simple classification
  problems defined on distributions of labeled assemblies, a new assembly representing
  each class can be reliably formed in response to a few stimuli from the class; this
  assembly is henceforth reliably recalled in response to new stimuli from the same
  class.  Furthermore, such class assemblies will be distinguishable as long as the
  respective classes are reasonably separated — for example, when they are clusters
  of similar assemblies, or more generally separable with margin by a linear threshold
  function. To prove these results, we draw on random graph theory with dynamic edge
  weights to estimate sequences of activated vertices, yielding strong generalizations
  of previous calculations and theorems in this field over the past five years. These
  theorems are backed up by experiments demonstrating the successful formation of
  assemblies which represent concept classes on synthetic data drawn from such distributions,
  and also on MNIST, which lends itself to classification through one assembly per
  digit. Seen as a learning algorithm, this mechanism is entirely online, generalizes
  from very few samples, and requires only mild supervision — all key attributes of
  learning in a model of the brain.  We argue that this learning mechanism, supported
  by separate sensory pre-processing mechanisms for extracting attributes, such as
  edges or phonemes, from real world data, can be the basis of biological learning
  in cortex.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: dabagia22a
month: 0
tex_title: Assemblies of neurons learn to classify well-separated distributions
firstpage: 3685
lastpage: 3717
page: 3685-3717
order: 3685
cycles: false
bibtex_author: Dabagia, Max and Vempala, Santosh S and Papadimitriou, Christos
author:
- given: Max
  family: Dabagia
- given: Santosh S
  family: Vempala
- given: Christos
  family: Papadimitriou
date: 2022-06-28
address:
container-title: Proceedings of Thirty Fifth Conference on Learning Theory
volume: '178'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v178/dabagia22a/dabagia22a.pdf
extras:
- label: Supplementary ZIP
  link: https://proceedings.mlr.press/v178/dabagia22a/dabagia22a-supp.zip
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
