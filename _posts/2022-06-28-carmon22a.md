---
title: Making SGD Parameter-Free
abstract: We develop an algorithm for parameter-free stochastic convex optimization
  (SCO) whose rate of convergence is only a double-logarithmic factor larger than
  the optimal rate for the corresponding known-parameter setting. In contrast, the
  best previously known rates for parameter-free SCO are based on online parameter-free
  regret bounds, which contain unavoidable excess logarithmic terms compared to their
  known-parameter counterparts. Our algorithm is conceptually simple, has high-probability
  guarantees, and is also partially adaptive to unknown gradient norms, smoothness,
  and strong convexity. At the heart of our results is a novel parameter-free certificate
  for SGD step size choice, and a time-uniform concentration result that assumes no
  a-priori bounds on SGD iterates.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: carmon22a
month: 0
tex_title: Making SGD Parameter-Free
firstpage: 2360
lastpage: 2389
page: 2360-2389
order: 2360
cycles: false
bibtex_author: Carmon, Yair and Hinder, Oliver
author:
- given: Yair
  family: Carmon
- given: Oliver
  family: Hinder
date: 2022-06-28
address:
container-title: Proceedings of Thirty Fifth Conference on Learning Theory
volume: '178'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v178/carmon22a/carmon22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
