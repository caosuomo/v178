---
title: 'Benign Overfitting without Linearity: Neural Network Classifiers Trained by
  Gradient Descent for Noisy Linear Data'
abstract: 'Benign overfitting, the phenomenon where interpolating models generalize
  well in the presence of noisy data, was first observed in neural network models
  trained with gradient descent.  To better understand this empirical observation,
  we consider the generalization error of two-layer neural networks trained to interpolation
  by gradient descent on the logistic loss following random initialization.  We assume
  the data comes from well-separated class-conditional log-concave distributions and
  allow for a constant fraction of the training labels to be corrupted by an adversary.  We
  show that in this setting, neural networks exhibit benign overfitting: they can
  be driven to zero training error, perfectly fitting any noisy training labels, and
  simultaneously achieve minimax optimal test error.   In contrast to previous work
  on benign overfitting that require linear or kernel-based predictors, our analysis
  holds in a setting where both the model and learning dynamics are fundamentally
  nonlinear.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: frei22a
month: 0
tex_title: 'Benign Overfitting without Linearity: Neural Network Classifiers Trained
  by Gradient Descent for Noisy Linear Data'
firstpage: 2668
lastpage: 2703
page: 2668-2703
order: 2668
cycles: false
bibtex_author: Frei, Spencer and Chatterji, Niladri S and Bartlett, Peter
author:
- given: Spencer
  family: Frei
- given: Niladri S
  family: Chatterji
- given: Peter
  family: Bartlett
date: 2022-06-28
address:
container-title: Proceedings of Thirty Fifth Conference on Learning Theory
volume: '178'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v178/frei22a/frei22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
