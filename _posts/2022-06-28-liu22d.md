---
title: On Almost Sure Convergence Rates of Stochastic Gradient Methods
abstract: The vast majority of convergence rates analysis for stochastic gradient
  methods in the literature focus on convergence in expectation, whereas trajectory-wise
  almost sure convergence is clearly important to ensure that any instantiation of
  the stochastic algorithms would converge with probability one. Here we provide a
  unified almost sure convergence rates analysis for stochastic gradient descent (SGD),
  stochastic heavy-ball (SHB), and stochastic Nesterovâ€™s accelerated gradient (SNAG)
  methods.  We show, for the first time, that the almost sure convergence rates obtained
  for these stochastic gradient methods on strongly convex functions, are arbitrarily
  close to their optimal convergence rates possible. For non-convex objective functions,
  we not only show that a weighted average of the squared gradient norms converges
  to zero almost surely, but also the last iterates of the algorithms. We further
  provide last-iterate almost sure convergence rates analysis for stochastic gradient
  methods on weakly convex smooth functions, in contrast with most existing results
  in the literature that only provide convergence in expectation for a weighted average
  of the iterates.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: liu22d
month: 0
tex_title: On Almost Sure Convergence Rates of Stochastic Gradient Methods
firstpage: 2963
lastpage: 2983
page: 2963-2983
order: 2963
cycles: false
bibtex_author: Liu, Jun and Yuan, Ye
author:
- given: Jun
  family: Liu
- given: Ye
  family: Yuan
date: 2022-06-28
address:
container-title: Proceedings of Thirty Fifth Conference on Learning Theory
volume: '178'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v178/liu22d/liu22d.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
