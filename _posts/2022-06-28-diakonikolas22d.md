---
title: Non-Gaussian Component Analysis via Lattice Basis Reduction
abstract: 'Non-Gaussian Component Analysis (NGCA) is the following distribution learning
  problem: Given i.i.d. samples from a distribution on $\R^d$ that is non-gaussian
  in a hidden direction $v$ and an independent standard Gaussian in the orthogonal
  directions, the goal is to approximate the hidden direction $v$. Prior work \citep{DKS17-sq}
  provided formal evidence for the existence of an information-computation tradeoff
  for NGCA under appropriate moment-matching conditions on the univariate non-gaussian
  distribution $A$. The latter result does not apply when the distribution $A$ is
  discrete. A natural question is whether information-computation tradeoffs persist
  in this setting. In this paper, we answer this question in the negative by obtaining
  a sample and computationally efficient algorithm for NGCA in the regime that $A$
  is discrete or nearly discrete, in a well-defined technical sense. The key tool
  leveraged in our algorithm is the LLL method \citep{LLL82} for lattice basis reduction.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: diakonikolas22d
month: 0
tex_title: Non-Gaussian Component Analysis via Lattice Basis Reduction
firstpage: 4535
lastpage: 4547
page: 4535-4547
order: 4535
cycles: false
bibtex_author: Diakonikolas, Ilias and Kane, Daniel
author:
- given: Ilias
  family: Diakonikolas
- given: Daniel
  family: Kane
date: 2022-06-28
address:
container-title: Proceedings of Thirty Fifth Conference on Learning Theory
volume: '178'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v178/diakonikolas22d/diakonikolas22d.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
