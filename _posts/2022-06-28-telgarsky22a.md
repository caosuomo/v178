---
title: Stochastic linear optimization never overfits with quadratically-bounded losses
  on general data
abstract: 'This work provides test error bounds for iterative fixed point methods
  on linear predictors — specifically, stochastic and batch mirror descent (MD), and
  stochastic temporal difference learning (TD) — with two core contributions: (a)
  a single proof technique which gives high probability guarantees despite the absence
  of projections, regularization, or any equivalents, even when optima have large
  or infinite norm, for quadratically-bounded losses (e.g., providing unified treatment
  of squared and logistic losses); (b) locally-adapted rates which depend not on global
  problem structure (such as conditions numbers and maximum margins), but rather on
  properties of low norm predictors which may suffer some small excess test error.
  The proof technique is an elementary and versatile coupling argument, and is demonstrated
  here in the following settings: stochastic MD under realizability; stochastic MD
  for general Markov data; batch MD for general IID data; stochastic MD on heavy-tailed
  data (still without projections); stochastic TD on approximately mixing Markov chains
  (all prior stochastic TD bounds are in expectation).'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: telgarsky22a
month: 0
tex_title: Stochastic linear optimization never overfits with quadratically-bounded
  losses on general data
firstpage: 5453
lastpage: 5488
page: 5453-5488
order: 5453
cycles: false
bibtex_author: Telgarsky, Matus
author:
- given: Matus
  family: Telgarsky
date: 2022-06-28
address:
container-title: Proceedings of Thirty Fifth Conference on Learning Theory
volume: '178'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v178/telgarsky22a/telgarsky22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
