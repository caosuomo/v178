---
title: How catastrophic can catastrophic forgetting be in linear regression?
abstract: To better understand catastrophic forgetting, we study fitting an overparameterized
  linear model to a sequence of tasks with different input distributions. We analyze
  how much the model forgets the true labels of earlier tasks after training on subsequent
  tasks, obtaining exact expressions and bounds. We establish connections between
  continual learning in the linear setting and two other research areas â€“  alternating
  projections and the Kaczmarz method. In specific settings, we highlight differences
  between forgetting and convergence to the offline solution as studied in those areas.   In
  particular, when $T$ tasks in $d$ dimensions are presented cyclically for $k$ iterations,
  we prove an upper bound of $T^2\min\{1/\sqrt{k},d/k\}$ on the forgetting. This stands
  in contrast to the convergence to the offline solution, which can be arbitrarily
  slow according to existing alternating projection results. We further show that
  the $T^2$ factor can be lifted when tasks are presented in a random ordering.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: evron22a
month: 0
tex_title: How catastrophic can catastrophic forgetting be in linear regression?
firstpage: 4028
lastpage: 4079
page: 4028-4079
order: 4028
cycles: false
bibtex_author: Evron, Itay and Moroshko, Edward and Ward, Rachel and Srebro, Nathan
  and Soudry, Daniel
author:
- given: Itay
  family: Evron
- given: Edward
  family: Moroshko
- given: Rachel
  family: Ward
- given: Nathan
  family: Srebro
- given: Daniel
  family: Soudry
date: 2022-06-28
address:
container-title: Proceedings of Thirty Fifth Conference on Learning Theory
volume: '178'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v178/evron22a/evron22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
