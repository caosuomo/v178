---
title: Uniform Stability for First-Order Empirical Risk Minimization
abstract: We consider the problem of designing uniformly stable first-order optimization
  algorithms for empirical risk minimization. Uniform stability is often used to obtain
  generalization error bounds for optimization algorithms, and we are interested in
  a general approach to achieve it. For Euclidean geometry, we suggest a black-box
  conversion which given a smooth optimization algorithm, produces a uniformly stable
  version of the algorithm while maintaining its convergence rate up to logarithmic
  factors. Using this reduction we obtain a (nearly) optimal algorithm for smooth
  optimization with convergence rate $\tilde{O}(1/T^2)$ and uniform stability $O(T^2/n)$,
  resolving an open problem of Chen et al. (2018); Attia and Koren (2021). For more
  general geometries, we develop a variant of Mirror Descent for smooth optimization
  with convergence rate $\tilde{O}(1/T)$ and uniform stability $O(T/n)$, leaving open
  the question of devising a general conversion method as in the Euclidean case.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: attia22a
month: 0
tex_title: Uniform Stability for First-Order Empirical Risk Minimization
firstpage: 3313
lastpage: 3332
page: 3313-3332
order: 3313
cycles: false
bibtex_author: Attia, Amit and Koren, Tomer
author:
- given: Amit
  family: Attia
- given: Tomer
  family: Koren
date: 2022-06-28
address:
container-title: Proceedings of Thirty Fifth Conference on Learning Theory
volume: '178'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v178/attia22a/attia22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
