---
title: Learning with metric losses
abstract: We propose a practical algorithm for learning mappings between two metric
  spaces, $\X$ and $\Y$. Our procedure is strongly Bayes-consistent whenever $\X$
  and $\Y$ are topologically separable and $\Y$ is “bounded in expectation” (our term;
  the separability assumption can be somewhat weakened). At this level of generality,
  ours is the first such learnability result for unbounded loss in the agnostic setting.
  Our technique is based on metric medoids (a variant of Fréchet means) and presents
  a significant departure from existing methods, which, as we demonstrate, fail to
  achieve Bayes-consistency on general instance- and label-space metrics. Our proofs
  introduce the technique of {\em semi-stable compression}, which may be of independent
  interest.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: cohen22a
month: 0
tex_title: Learning with metric losses
firstpage: 662
lastpage: 700
page: 662-700
order: 662
cycles: false
bibtex_author: Cohen, Dan Tsir and Kontorovich, Aryeh
author:
- given: Dan Tsir
  family: Cohen
- given: Aryeh
  family: Kontorovich
date: 2022-06-28
address:
container-title: Proceedings of Thirty Fifth Conference on Learning Theory
volume: '178'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v178/cohen22a/cohen22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
