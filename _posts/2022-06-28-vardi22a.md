---
title: Width is Less Important than Depth in ReLU Neural Networks
abstract: We solve an open question from Lu et al. (2017), by showing that any target
  network with inputs in $\mathbb{R}^d$ can be approximated by a width $O(d)$ network
  (independent of the target networkâ€™s architecture), whose number of parameters is
  essentially larger only by a linear factor. In light of previous depth separation
  theorems, which imply that a similar result cannot hold when the roles of width
  and depth are interchanged, it follows that depth plays a more significant role
  than width in the expressive power of neural networks. We extend our results to
  constructing networks with bounded weights, and to constructing networks with width
  at most $d+2$, which is close to the minimal possible width due to previous lower
  bounds. Both of these constructions cause an extra polynomial factor in the number
  of parameters over the target network. We also show an exact representation of wide
  and shallow networks using deep and narrow networks which, in certain cases, does
  not increase the number of parameters over the target network.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: vardi22a
month: 0
tex_title: Width is Less Important than Depth in ReLU Neural Networks
firstpage: 1249
lastpage: 1281
page: 1249-1281
order: 1249
cycles: false
bibtex_author: Vardi, Gal and Yehudai, Gilad and Shamir, Ohad
author:
- given: Gal
  family: Vardi
- given: Gilad
  family: Yehudai
- given: Ohad
  family: Shamir
date: 2022-06-28
address:
container-title: Proceedings of Thirty Fifth Conference on Learning Theory
volume: '178'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v178/vardi22a/vardi22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
