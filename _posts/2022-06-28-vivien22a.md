---
title: Label noise (stochastic) gradient descent implicitly solves the Lasso for quadratic
  parametrisation
abstract: Understanding the implicit bias of training algorithms is of crucial importance
  in order to explain the success of overparametrised neural networks. In this paper,
  we study the role of the label noise in the training dynamics of a quadratically
  parametrised model through its continuous time version. We explicitly characterise
  the solution chosen by the stochastic flow and prove that it implicitly solves a
  Lasso program. To fully complete our analysis, we provide nonasymptotic convergence
  guarantees for the dynamics as well as conditions for support recovery. We also
  give experimental results which support our theoretical claims. Our findings highlight
  the fact that structured noise can induce better generalisation and help explain
  the greater performances of stochastic dynamics as observed in practice.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: vivien22a
month: 0
tex_title: Label noise (stochastic) gradient descent implicitly solves the Lasso for
  quadratic parametrisation
firstpage: 2127
lastpage: 2159
page: 2127-2159
order: 2127
cycles: false
bibtex_author: Vivien, Loucas Pillaud and Reygner, Julien and Flammarion, Nicolas
author:
- given: Loucas Pillaud
  family: Vivien
- given: Julien
  family: Reygner
- given: Nicolas
  family: Flammarion
date: 2022-06-28
address:
container-title: Proceedings of Thirty Fifth Conference on Learning Theory
volume: '178'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v178/vivien22a/vivien22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
