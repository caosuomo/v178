---
title: 'ROOT-SGD: Sharp Nonasymptotics and Asymptotic Efficiency in a Single Algorithm'
abstract: We study the problem of solving strongly convex and smooth unconstrained
  optimization problems using stochastic first-order algorithms. We devise a novel
  algorithm, referred to as \emph{Recursive One-Over-T SGD} (ROOT-SGD), based on an
  easily implementable, recursive averaging of past stochastic gradients. We prove
  that it simultaneously achieves state-of-the-art performance in both a finite-sample,
  nonasymptotic sense and an asymptotic sense. On the nonasymptotic side, we prove
  risk bounds on the last iterate of ROOT-SGD with leading-order terms that match
  the optimal statistical risk with a unity pre-factor, along with a higher-order
  term that scales at the sharp rate of $O(n^{-3/2})$ under the Lipschitz condition
  on the Hessian matrix. On the asymptotic side, we show that when a mild, one-point
  Hessian continuity condition is imposed, the rescaled last iterate of (multi-epoch)
  ROOT-SGD converges asymptotically to a Gaussian limit with the Cram√©r-Rao optimal
  asymptotic covariance, for a broad range of step-size choices.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: li22a
month: 0
tex_title: 'ROOT-SGD: Sharp Nonasymptotics and Asymptotic Efficiency in a Single Algorithm'
firstpage: 909
lastpage: 981
page: 909-981
order: 909
cycles: false
bibtex_author: Li, Chris Junchi and Mou, Wenlong and Wainwright, Martin and Jordan,
  Michael
author:
- given: Chris Junchi
  family: Li
- given: Wenlong
  family: Mou
- given: Martin
  family: Wainwright
- given: Michael
  family: Jordan
date: 2022-06-28
address:
container-title: Proceedings of Thirty Fifth Conference on Learning Theory
volume: '178'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v178/li22a/li22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
