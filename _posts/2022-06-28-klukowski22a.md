---
title: Rate of Convergence of Polynomial Networks to Gaussian Processes
abstract: We examine one-hidden-layer neural networks with random weights. It is well-known
  that in the limit of infinitely many neurons they simplify to Gaussian processes.
  For networks with a polynomial activation, we demonstrate that the rate of this
  convergence in 2-Wasserstein metric is O(1/sqrt(n)), where n is the number of hidden
  neurons. We suspect this rate is asymptotically sharp. We improve the known convergence
  rate for other activations, to power-law in n for ReLU and inverse-square-root up
  to logarithmic factors for erf. We explore the interplay between spherical harmonics,
  Stein kernels and optimal transport in the non-isotropic setting.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: klukowski22a
month: 0
tex_title: Rate of Convergence of Polynomial Networks to Gaussian Processes
firstpage: 701
lastpage: 722
page: 701-722
order: 701
cycles: false
bibtex_author: Klukowski, Adam
author:
- given: Adam
  family: Klukowski
date: 2022-06-28
address:
container-title: Proceedings of Thirty Fifth Conference on Learning Theory
volume: '178'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v178/klukowski22a/klukowski22a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
